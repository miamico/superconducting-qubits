{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8913428",
   "metadata": {},
   "source": [
    "# Optimizing π Pulses for Superconducting Qubits Using Reinforcement Learning with JAX and QuTiP\n",
    "\n",
    "This tutorial provides a step-by-step guide to implementing reinforcement learning (RL) for optimizing microwave drive pulses in a superconducting qubit system. Specifically, we focus on calibrating a π pulse—a pulse that flips the qubit from its ground state |0⟩ to the excited state |1⟩—while minimizing leakage to higher levels like |2⟩. The system is modeled using circuit quantum electrodynamics (cQED), where a transmon qubit is coupled to a resonator. We use the Schrieffer-Wolff (SW) transformation to derive an effective Hamiltonian for the low-energy subspace, reducing computational complexity.\n",
    "\n",
    "The approach leverages:\n",
    "- **QuTiP** (with JAX backend via `qutip-jax`) for quantum simulations.\n",
    "- **JAX** for automatic differentiation and efficient numerics.\n",
    "- **REINFORCE** (a policy gradient RL algorithm) to learn piece-wise constant pulse shapes.\n",
    "\n",
    "By the end, you'll understand how to simulate qubit dynamics, apply perturbation theory (SW transform), and use RL to optimize controls. We'll derive key concepts mathematically, like a proof assistant, guiding you through each logical step.\n",
    "\n",
    "## Prerequisites\n",
    "Before diving in, ensure you have:\n",
    "- **Libraries**: Install via pip: `jax`, `jaxlib`, `qutip`, `qutip-jax`, `optax`.\n",
    "- **Basic Knowledge**:\n",
    "  - Quantum mechanics: Hamiltonians, operators, time evolution (Schrödinger equation).\n",
    "  - RL: Policies, rewards, policy gradients (REINFORCE).\n",
    "  - JAX: Autodiff, JIT compilation, vectorization (`vmap`).\n",
    "- **Environment**: Python 3.12+ with GPU support for JAX if available (speeds up training).\n",
    "\n",
    "If you're new:\n",
    "- Read QuTiP docs for quantum objects (`Qobj`).\n",
    "- JAX tutorial for `grad`, `jit`, `vmap`.\n",
    "- RL intro: REINFORCE maximizes expected rewards by adjusting policy parameters via gradients.\n",
    "\n",
    "**Note**: If you encounter `TypeError` with `Qobj` (e.g., JAX not recognizing QuTiP objects), convert explicitly: `jnp.asarray(qobj.full())`.\n",
    "\n",
    "## Section 1: Schrieffer-Wolff Transformation for Effective Hamiltonians\n",
    "\n",
    "### Theory and Derivation\n",
    "In cQED, the full Hamiltonian for a transmon qubit coupled to a resonator is complex due to infinite levels. The SW transformation is a unitary perturbation method to block-diagonalize it, yielding an effective low-energy Hamiltonian.\n",
    "\n",
    "Start with the general form:\n",
    "$$ H = H_0 + V $$\n",
    "where $ H_0 $ is the unperturbed (diagonal) part, and $ V $ is the perturbation (off-diagonal coupling).\n",
    "\n",
    "**Goal**: Find a unitary $ U = e^S $ (with anti-Hermitian $ S $) such that $ H' = U^\\dagger H U $ is block-diagonal, decoupling subspaces.\n",
    "\n",
    "Derive $ S $ step-by-step:\n",
    "\n",
    "1. Assume $ S $ is small, expand $ e^S \\approx 1 + S + \\frac{1}{2} S^2 + \\cdots $.\n",
    "\n",
    "2. The transformed Hamiltonian:\n",
    "$$ H' = e^{-S} H e^S = H + [H, S] + \\frac{1}{2} [[H, S], S] + \\cdots $$\n",
    "(Baker-Campbell-Hausdorff expansion).\n",
    "\n",
    "3. Split $ H_0 $ into subspaces A (low-energy) and B (high-energy). $ V $ couples them.\n",
    "\n",
    "4. Choose $ S $ so off-diagonal terms vanish to first order: $ [H_0, S] + V = 0 $ (off-diagonal).\n",
    "\n",
    "   - Matrix elements: For $ i \\in A, j \\in B $, $ S_{ij} = \\frac{V_{ij}}{E_j - E_i} $ (from commutator).\n",
    "\n",
    "   - Proof: $ [H_0, S]_{ij} = (E_i - E_j) S_{ij} $, so $ (E_i - E_j) S_{ij} + V_{ij} = 0 $ implies $ S_{ij} = \\frac{V_{ij}}{E_j - E_i} $.\n",
    "\n",
    "5. Higher orders: Iterate commutators up to order 4 (as in code) for accuracy.\n",
    "\n",
    "For our system:\n",
    "- $ H_0 = \\omega_c a^\\dagger a + \\omega_q b^\\dagger b + \\frac{\\alpha}{2} b^\\dagger b (b^\\dagger b - 1) $ (resonator + anharmonic qubit).\n",
    "- $ V = g (a b^\\dagger + a^\\dagger b) $ (RWA coupling).\n",
    "\n",
    "The effective Hamiltonian includes dispersive shift $ \\chi b^\\dagger b a^\\dagger a $.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4caa406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad, vmap, random\n",
    "from jax.experimental.ode import odeint\n",
    "import qutip_jax\n",
    "from qutip import Qobj, tensor, destroy, qeye, basis\n",
    "import optax\n",
    "\n",
    "# SW functions\n",
    "def commutator(A, B):\n",
    "    return A * B - B * A\n",
    "\n",
    "def compute_generator_S(H0, V):\n",
    "    energies = H0.diag()  # Eigenenergies of H0\n",
    "    dim = H0.shape[0]\n",
    "    V_mat = V.full()  # Dense matrix\n",
    "    i, j = jnp.meshgrid(jnp.arange(dim), jnp.arange(dim), indexing='ij')\n",
    "    delta = energies[i] - energies[j]\n",
    "    cond = (jnp.abs(delta) > 1e-12) & (i != j)  # Avoid zero division\n",
    "    S_mat = jnp.where(cond, V_mat[i, j] / delta, 0)\n",
    "    return Qobj(S_mat, dims=H0.dims, dtype=\"jax\")\n",
    "\n",
    "def effective_hamiltonian(H, S, order=4):\n",
    "    H_eff = Qobj(jnp.zeros_like(H.full()), dims=H.dims, dtype=\"jax\")\n",
    "    current_term = H.copy()\n",
    "    H_eff += current_term\n",
    "    fact = 1.0\n",
    "    for k in range(1, order + 1):\n",
    "        current_term = commutator(S, current_term)\n",
    "        fact *= k\n",
    "        H_eff += current_term / fact\n",
    "    return H_eff\n",
    "\n",
    "def transformed_operator(O, S, order=4):\n",
    "    O_eff = O.copy()\n",
    "    current_term = O.copy()\n",
    "    fact = 1.0\n",
    "    for k in range(1, order + 1):\n",
    "        current_term = commutator(S, current_term)\n",
    "        fact *= k\n",
    "        O_eff += current_term / fact\n",
    "    return O_eff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee5ffb",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- `compute_generator_S`: Builds $ S $ element-wise. $ \\delta = E_i - E_j $, $ S_{ij} = V_{ij} / \\delta $ for $ i \\neq j $.\n",
    "- `effective_hamiltonian`: BCH expansion to order 4: $ H_{\\text{eff}} = H + [S, H] + \\frac{1}{2} [S, [S, H]] + \\cdots $.\n",
    "- `transformed_operator`: Similarly transforms drive operators.\n",
    "\n",
    "## Section 2: System Parameters and Effective Model Setup\n",
    "\n",
    "### Theory and Derivation\n",
    "Define the system:\n",
    "- Resonator frequency $ \\omega_c = 5.0 $, qubit $ \\omega_q = 6.0 $, anharmonicity $ \\alpha = -0.3 $, coupling $ g = 0.1 $.\n",
    "- Hilbert space: Resonator truncated to $ N_c = 10 $ levels, qubit to $ N_q = 5 $.\n",
    "\n",
    "Operators:\n",
    "- $ a, a^\\dagger $: Resonator lowering/raising.\n",
    "- $ b, b^\\dagger $: Qubit (anharmonic oscillator).\n",
    "\n",
    "Hamiltonian derivation:\n",
    "1. Resonator: $ H_c = \\omega_c a^\\dagger a $.\n",
    "2. Qubit: $ H_q = \\omega_q b^\\dagger b + \\frac{\\alpha}{2} b^\\dagger b (b^\\dagger b - 1) $ (Duffing oscillator approximation for transmon).\n",
    "3. Coupling: $ V = g (a b^\\dagger + a^\\dagger b) $ (rotating-wave approx.).\n",
    "\n",
    "Apply SW:\n",
    "- Compute $ S $, then $ H_{\\text{eff}} $.\n",
    "- Extract parameters: Qubit freq $ \\omega_{d_q} = E_{01} - E_g $, dispersive $ \\chi = (E_{11} - E_{01}) - \\omega_m $.\n",
    "\n",
    "Reduce to 3-level qubit subspace (|0⟩, |1⟩, |2⟩) for leakage-aware simulation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd25e203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SW effective: qubit freq 6.009900222, cavity freq (g) 4.990099778, chi -0.0080571479285787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2x/mrqhlfzs6ss_76w_zk3dlmmh0000gn/T/ipykernel_46385/1697537882.py:20: RuntimeWarning: invalid value encountered in divide\n",
      "  S_mat = jnp.where(cond, V_mat[i, j] / delta, 0)\n"
     ]
    }
   ],
   "source": [
    "# System parameters\n",
    "omega_c = 5.0\n",
    "omega_q = 6.0\n",
    "alpha = -0.3\n",
    "g = 0.1\n",
    "N_c = 10\n",
    "N_q = 5\n",
    "\n",
    "# Operators and Hamiltonians\n",
    "a = tensor(destroy(N_c, dtype=\"jax\"), qeye(N_q, dtype=\"jax\"))\n",
    "ad = a.dag()\n",
    "b = tensor(qeye(N_c, dtype=\"jax\"), destroy(N_q, dtype=\"jax\"))\n",
    "bd = b.dag()\n",
    "num_c = ad * a\n",
    "num_q = bd * b\n",
    "\n",
    "H_c = omega_c * num_c\n",
    "H_q = omega_q * num_q + (alpha / 2.0) * num_q * (num_q - 1)\n",
    "H0 = H_c + H_q\n",
    "V = g * (a * bd + ad * b)\n",
    "H = H0 + V\n",
    "\n",
    "# Compute SW\n",
    "S = compute_generator_S(H0, V)\n",
    "H_eff = effective_hamiltonian(H, S, order=8)\n",
    "\n",
    "diag = H_eff.diag()\n",
    "E_g = diag[0]\n",
    "E_01 = diag[1]\n",
    "E_10 = diag[N_q]\n",
    "E_11 = diag[N_q + 1]\n",
    "omega_d_q = float(E_01 - E_g)\n",
    "omega_m = float(E_10 - E_g)\n",
    "chi = float((E_11 - E_01) - omega_m)\n",
    "print(f\"SW effective: qubit freq {omega_d_q}, cavity freq (g) {omega_m}, chi {chi}\")\n",
    "\n",
    "# Effective qubit subspace\n",
    "H_q_eff_mat = jnp.diag(diag[:3] - E_g)\n",
    "H_q_eff = Qobj(H_q_eff_mat, dims=[[3], [3]], dtype=\"jax\")\n",
    "\n",
    "b_eff = transformed_operator(b + bd, S, order=4)\n",
    "b_q_eff_mat = b_eff.full()[:3, :3]\n",
    "b_q_eff = Qobj(b_q_eff_mat, dims=[[3], [3]], dtype=\"jax\")\n",
    "\n",
    "bq = destroy(3, dtype=\"jax\")\n",
    "num_q_q = bq.dag() * bq\n",
    "num_q2_op = num_q_q * (num_q_q - qeye(3, dtype=\"jax\")) / 2.0\n",
    "\n",
    "psi0_q = basis(3, 0, dtype=\"jax\").full().flatten()\n",
    "target_state = basis(3, 1, dtype=\"jax\").full().flatten()  # Target for pi pulse\n",
    "\n",
    "H0_mat = H_q_eff.full()\n",
    "drive_mat = b_q_eff.full()\n",
    "num_q_mat = num_q_q.full()\n",
    "num_q2_mat = num_q2_op.full()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d88ecb8",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- Operators are tensor products (e.g., $ a $ acts on resonator, identity on qubit).\n",
    "- SW extracts effective parameters: Print shows $ \\omega_{d_q} \\approx 6.0 $, $ \\chi \\approx -0.001 $ (weak coupling).\n",
    "- Subspace: Focus on first 3 qubit levels to track leakage ($ P_2 = \\langle 2 | \\rho | 2 \\rangle $).\n",
    "- States: $ \\psi_0 = |0\\rangle $, target $ |1\\rangle $.\n",
    "\n",
    "## Section 3: Reinforcement Learning Setup\n",
    "\n",
    "### Theory and Derivation\n",
    "RL optimizes the pulse by treating it as a sequential decision process:\n",
    "- **Environment**: Qubit state evolves under piecewise-constant drives.\n",
    "- **Agent (Policy)**: Neural net outputs amplitude distribution for each bin.\n",
    "- **Reward**: Final fidelity to |1⟩ (minus leakage).\n",
    "\n",
    "REINFORCE derivation:\n",
    "1. Policy $ \\pi_\\theta(a|s) $: Gaussian $ \\mathcal{N}(\\mu(s), \\sigma(s)) $, params $ \\theta $.\n",
    "2. Trajectory $ \\tau = (s_0, a_0, r_0, \\dots, s_T) $, return $ G = \\sum r_t $.\n",
    "3. Objective: $ J(\\theta) = \\mathbb{E}_\\tau [G] $.\n",
    "4. Gradient: $ \\nabla J = \\mathbb{E} [G \\nabla \\log P(\\tau)] $ (policy gradient theorem).\n",
    "5. Baseline: Subtract mean reward to reduce variance: advantage $ A = G - b $.\n",
    "\n",
    "For our case:\n",
    "- State $ s $: Flattened Re/Im of $ \\psi $ (6D vector).\n",
    "- Action $ a $: Scalar amplitude per bin, scaled by 0.05.\n",
    "- Pulse: $ \\epsilon(t) = a_k \\cos(\\omega_{d_q} t) $ in bin k.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ea1ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL Hyperparameters\n",
    "K = 10  # Number of pulse segments (time bins)\n",
    "tau_total = 100.0  # Fixed total pulse duration\n",
    "dt = tau_total / K  # Duration per bin\n",
    "state_dim = 6  # Flattened real + imag of psi (3D complex)\n",
    "action_dim = 1  # Amplitude per bin (scalar)\n",
    "hidden_dim = 64\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "action_scale = 0.05  # Scale actions to match epsilon_q ~0.05\n",
    "\n",
    "# Policy network: MLP for mean and log_std of Gaussian\n",
    "def policy_network(params, state):\n",
    "    hidden = jnp.tanh(jnp.dot(params['w1'], state) + params['b1'])\n",
    "    mean = jnp.dot(params['w2'], hidden) + params['b2']\n",
    "    log_std = jnp.dot(params['w3'], hidden) + params['b3']\n",
    "    return mean, log_std\n",
    "\n",
    "# Initialize params\n",
    "key = random.PRNGKey(0)\n",
    "keys = random.split(key, 4)\n",
    "params = {\n",
    "    'w1': random.normal(keys[0], (hidden_dim, state_dim)) * 0.1,\n",
    "    'b1': jnp.zeros(hidden_dim),\n",
    "    'w2': random.normal(keys[1], (action_dim, hidden_dim)) * 0.1,\n",
    "    'b2': jnp.zeros(action_dim),\n",
    "    'w3': random.normal(keys[2], (action_dim, hidden_dim)) * 0.1,\n",
    "    'b3': jnp.zeros(action_dim)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fee9a9e",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation**:\n",
    "- Hyperparams: K=10 bins over 100 units time (arbitrary units).\n",
    "- Policy: Single hidden layer MLP. $ \\mu = w_2 \\cdot \\tanh(w_1 s + b_1) + b_2 $, $ \\log \\sigma $ similar.\n",
    "- Init: Small random weights (scale 0.1) for stability.\n",
    "\n",
    "## Section 4: Dynamics Simulation and Episode Rollout\n",
    "\n",
    "### Theory and Derivation\n",
    "Time evolution: Solve Schrödinger equation $ i \\dot{\\psi} = H(t) \\psi $, with $ H(t) = H_{\\text{eff}} + \\epsilon(t) (b + b^\\dagger) $.\n",
    "\n",
    "For real-valued ODE:\n",
    "1. $ \\psi = \\psi_r + i \\psi_i $, flatten to vector y = [ψ_r, ψ_i].\n",
    "2. $ \\dot{y} = [\\Re(-i H \\psi), \\Im(-i H \\psi)] $.\n",
    "\n",
    "Per bin: Constant amplitude, integrate with `odeint`.\n",
    "\n",
    "Episode:\n",
    "- Start at s0 = flatten(ψ0).\n",
    "- For each bin: Sample a ~ π(s), evolve, collect log π(a|s).\n",
    "- Reward only at end: Fidelity $ F = |\\langle 1 | \\psi_f \\rangle|^2 $.\n",
    "\n",
    "Batch via `vmap` for parallel episodes.\n",
    "\n",
    "### Code Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ded8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ODE for one time bin (constant amplitude in bin)\n",
    "def evolve_bin(y, t_start, amplitude, omega_d_q):\n",
    "    def schrodinger_real_bin(y, t, H0_mat, drive_mat, amp, omega_d_q):\n",
    "        psi_real = y[:3]\n",
    "        psi_imag = y[3:]\n",
    "        psi = psi_real + 1j * psi_imag\n",
    "        drive = amp * jnp.cos(omega_d_q * (t + t_start))  # Continue time\n",
    "        H = H0_mat + drive * drive_mat\n",
    "        dpsi_dt = -1j * jnp.dot(H, psi)\n",
    "        return jnp.concatenate([jnp.real(dpsi_dt), jnp.imag(dpsi_dt)])\n",
    "    \n",
    "    t_bin = jnp.linspace(0, dt, 10)  # Fine-grained for accuracy\n",
    "    return odeint(schrodinger_real_bin, y, t_bin, H0_mat, drive_mat, amplitude, omega_d_q)[-1]\n",
    "\n",
    "# Simulate one episode: sequential actions over K bins\n",
    "def simulate_episode(params, key):\n",
    "    state = jnp.concatenate([jnp.real(psi0_q), jnp.imag(psi0_q)])\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    t_current = 0.0\n",
    "    \n",
    "    for k in range(K):\n",
    "        mean, log_std = policy_network(params, state)\n",
    "        std = jnp.exp(log_std)\n",
    "        key, subkey = random.split(key)\n",
    "        action = mean + std * random.normal(subkey, (action_dim,))\n",
    "        amplitude = action_scale * action[0]  # Scale action\n",
    "        \n",
    "        # Evolve for this bin\n",
    "        new_state = evolve_bin(state, t_current, amplitude, omega_d_q)\n",
    "        t_current += dt\n",
    "        \n",
    "        # Log prob\n",
    "        normal = (action - mean) / std\n",
    "        log_prob = -0.5 * (jnp.log(2 * jnp.pi) + 2 * log_std + normal**2)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        # Intermediate reward 0\n",
    "        rewards.append(0.0)\n",
    "        \n",
    "        state = new_state\n",
    "    \n",
    "    # Final reward\n",
    "    psi_final_real = state[:3]\n",
    "    psi_final_imag = state[3:]\n",
    "    psi_final = psi_final_real + 1j * psi_final_imag\n",
    "    fidelity = jnp.abs(jnp.dot(jnp.conj(target_state), psi_final))**2\n",
    "    P2 = jnp.real(jnp.dot(jnp.conj(psi_final), jnp.dot(num_q2_mat, psi_final)))\n",
    "    final_reward = fidelity #- 10 * P2  # High penalty for leakage (commented in code)\n",
    "    rewards[-1] = final_reward\n",
    "    \n",
    "    return jnp.sum(jnp.array(rewards)), jnp.sum(jnp.array(log_probs)), psi_final\n",
    "\n",
    "# Vectorize over batch\n",
    "batch_simulate = vmap(simulate_episode, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996ea178",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- `evolve_bin`: Solves ODE for bin. Drive $ \\epsilon(t) = \\text{amp} \\cos(\\omega t) $.\n",
    "- Episode loop: Sequential (not vectorized, but batches via vmap).\n",
    "- Log prob: For Gaussian, $ \\log \\pi = -\\frac{1}{2} [\\log(2\\pi) + 2\\log\\sigma + ((a-\\mu)/\\sigma)^2] $.\n",
    "- Reward: Sum is just final F (intermediates 0). Uncomment penalty for leakage.\n",
    "\n",
    "## Section 5: Training and Evaluation\n",
    "\n",
    "### Theory and Derivation\n",
    "Loss: $ L = - \\mathbb{E} [A \\log \\pi] $, with A = G - baseline.\n",
    "\n",
    "1. Compute over batch: Rewards, log probs.\n",
    "2. Baseline = mean(rewards).\n",
    "3. Gradient ascent on J via Adam optimizer.\n",
    "\n",
    "Evaluation: Run deterministic episode (use means, no sampling) to compute P1 = <1|ρ|1>, P2 = <2|ρ|2>.\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa08673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.14163537202737764, Avg Reward: 0.32457638922913906\n",
      "Optimized P1: 0.056888330325838087, P2: 0.024706122554183724\n"
     ]
    }
   ],
   "source": [
    "# Loss for REINFORCE\n",
    "def loss_fn(params, keys):\n",
    "    rewards, log_probs, _ = batch_simulate(params, keys)\n",
    "    baseline = jnp.mean(rewards)\n",
    "    advantages = rewards - baseline\n",
    "    return -jnp.mean(advantages * log_probs)\n",
    "\n",
    "grad_loss = jit(grad(loss_fn))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    key = random.PRNGKey(epoch)\n",
    "    keys = random.split(key, batch_size)\n",
    "    loss = loss_fn(params, keys)\n",
    "    grads = grad_loss(params, keys)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        rewards, _, _ = batch_simulate(params, keys)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}, Avg Reward: {jnp.mean(rewards)}\")\n",
    "\n",
    "# Evaluate optimal policy (deterministic: use means)\n",
    "_, _, psi_final = simulate_episode(params, random.PRNGKey(42))  # Run one episode\n",
    "P1_opt = jnp.real(jnp.dot(jnp.conj(psi_final), jnp.dot(num_q_mat, psi_final)))\n",
    "P2_opt = jnp.real(jnp.dot(jnp.conj(psi_final), jnp.dot(num_q2_mat, psi_final)))\n",
    "print(f\"Optimized P1: {P1_opt}, P2: {P2_opt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db66db57",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- `loss_fn`: Averages over batch.\n",
    "- Training: 50 epochs, print every 50 (adjust for longer runs).\n",
    "- Eval: Uses same simulate but with fixed key; P1/P2 via expectation values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
